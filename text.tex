\documentclass[a4paper,14pt]{article}
\linespread{1.5}

\usepackage{cmap}					% поиск в PDF
\usepackage[T2A]{fontenc}			% кодировка
\usepackage[utf8]{inputenc}			% кодировка исходного текста
\usepackage[english,russian]{babel}	% локализация и переносы
\usepackage{listings}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\author{Anastasia Nichiporchuk}
\title{Drug Discovery via Graph Neural Networks}
\date{\today}

\begin{document} % Конец преамбулы, начало текста.

\maketitle

\section*{Abstract}
The paper considers a method for predicting solubility in water for various molecules using graph neural networks. The architecture of the neural network is given, neural network training on the AqSol dataset is considered, a test is conducted on the ESOL benchmark.

// TODO: дописать про генеративную часть

\section{Introduction}\label{1}

The need to treat illness and maintain health has accompanied humans throughout their existence as a species. The development of the pharmaceutical industry has improved the quality of life, increased its duration, and made it possible to cope with diseases that were previously considered incurable. The development of vaccines for preventive protection against disease, both for the individual and for society as a whole, as demonstrated by the recent COVID-19 pandemic, is also crucial. 

Modern drug development is inextricably linked to the use of various technologies. Pharmaceutical companies compete for primacy in the development of drugs for diseases that place the greatest burden on the health care system. Before the discovery of penicillin, these were mostly diseases of an infectious nature. Nowadays, cardiovascular diseases, cancer and neurodegenerative diseases account for the largest percentage.

Because time is of the essence in drug development, pharmaceutical companies use a variety of cutting-edge research methods. In addition to laboratory research, computer simulations of different processes, mathematical modeling, and other methods that can reduce the amount of laboratory research are actively used. 


This is due to the fact that conducting any laboratory research requires time and financial expenses. Also, you need highly qualified specialists who are able to conduct this research. For example, a substance can be tested in the laboratory for a fairly small number of properties, so you need to try empirically to exclude those tests that are known to give a bad result.

With the development of computing power, as well as biotechnology and cheminformatics, it has become possible to use neural networks as such empirical methods. A neural network trained for a specific task can predict certain data and minimize unsuccessful laboratory tests.

\section{Solubility}\label{2}
Solubility is the maximum amount of solute that can be dissolved in a known amount of solvent at a particular temperature.

Factors affecting solubility:
\begin {itemize}
\item {temperature. Solubility depends significantly on temperature and can be changed by increasing or lowering the temperature. As a rule, the range from 20°C to 100 °C is considered suitable for dissolution in water.}

\item{pressure. Pressure has practically no effect on the solubility of solids, but for gaseous substances, solubility is directly proportional to the pressure of the gas over the liquid.}

\item{the nature of the solvent and the solute. Water belongs to polar solvents and, accordingly, dissolves polar substances better.}
\end {itemize}

Aqueous solubility, denoted by S, or its logarithm value log Is very important molecular property. Identification of
molecules with undesirable solubility in water at early stages is of great importance in drug development and in other related fields of pharmacy, since solubility affects the processes of absorption, distribution, metabolism and elimination (ADME).

A number of existing QSPR models developed to predict solubility show that solubility is related to experimental indicators such as melting point and separation coefficient.
However, the data obtained experimentally often contain measurement errors. This increases the difficulty of predicting solubility.

The most using formula of solubility is:
\begin {equation}
 logS =  0.5 - 0.01(MP - 25) - logP,
\end{equation}
where MP is melting point and logP is the log of the octanol-water partition coefficient.

Both partition coefficient and aqueous solubility reveal how
absolute dissolves in a solvent.

Solubility of molecules in water is an important property in terms of drug development. Substances with good solubility can be incorporated into drugs "as is," while substances with low solubility require the addition of various impurities to increase the final solubility. 

The most popular forms of drugs are tablets and solutions for injection. It is not uncommon for laboratory tests to find a substance that has the desired effect (capable of acting as a medicine for a specific disease). Such a substance needs to be submitted for clinical trials already in the dosage form in which it is supposed to be taken by patients. At this stage there may be a hitch, precisely because the active ingredient is poorly soluble in water, and it is not technologically easy to make a tablet from it.

Solubility can also affect the absorption of the active ingredient and its stability while in the dosage form. Thus, it can be determined that the better the solubility of the substance in water, the easier and cheaper it will be to use to make the drug.

\section{Graph Neural Networks}\label{3}

Graph neural networks are a class of neural networks that allow you to apply deep learning techniques to a variety of data that can be represented as a graph. Graph data has no regular structure, like pictures (it is known exactly how many neighbors a pixel has) or texts (there is a sequence of words in a sentence). Therefore, classical neural networks are not well suited to this kind of data. At the same time, the variety of such data in the real world is very large - as an example, we can consider social networks, routes, and molecular structures. 

There are several different types of graph problems:
\begin{enumerate}
  \item node-level prediction
  \item edge-level prediction
  \item graph-level prediction
  \item community detection (the problem of searching for clusters in a graph).
\end{enumerate}

The first type can include various tasks of prediction and classification of graph vertices, for example, prediction of the topic of an article on the basis of citations. 

The second type includes tasks related to a pair of vertices or edges, the so-called "link prediction" tasks. As an example of such a problem, we can consider recommendation systems that predict whether a user is suitable for a service offered. 

The third type, graph prediction, involves problems where a graph is treated as an object and the output is an estimate of its property. These can be both classification and regression problems. Such problems are often used when considering molecules to predict their properties.

Clustering tasks belong to unsupervised learning and can be used, for example, to distinguish groups of familiar users in social networks.

Below we will consider in more detail in which areas graph neural networks are used.

\subsection{Graph neural networks in different tasks}\label{4}

\subsubsection*{Computer vision}

In computer vision classical neural network models are actively used, in particular CNN. However, if we consider an image as a graph, in which each pixel is a vertex of the graph, and the neighborhood with other pixels determines the presence of edges between the corresponding vertices, it becomes possible to apply GNN for appropriate tasks related to images.

In addition, graphs are actively used for more complex tasks involving relations between image objects.
One such task is scene graph generation, in which the goal of the model is to parse an image into a semantic graph consisting of objects and their semantic relations. From the image data, the model detects and recognizes objects and predicts semantic relationships between pairs of objects.

Another important task is to match objects in scenes. It makes it possible to compile three-dimensional models from the available video footage, which are used for the modeling and mapping (SLAM). If certain hardware conditions are met, such models can also work in real time.

\subsubsection*{Natural language processing}
In natural language processing it is possible to represent a sequence of words as a simple graph. Such a format seems quite intuitive, but not much in demand. The representation of natural language in the form of graphs, such as dependency graphs, constituency graphs, AMR graphs and knowledge graphs, as well as text graphs containing multiple hierarchies of elements, i.e. document, sentence and word.

// TODO: дописать про комбинацию NLP+GNN

\subsubsection*{Recommendation systems}
Recommendation systems are designed to analyze user interests and predict what will be interesting to a particular user at the moment.

Recommendation systems are divided into two fundamentally different types - off-line models (detection of global patterns, personalized model for a specific object) and online models (fast response, detection of current actual trends). 

Graphs are actively used for offline models. are used by large companies in their products (Alibaba, Uber, Pinterest) to prepare personalized offers for users based on their actions and reactions. In this case, the user and the product can be represented as vertices of the graph, and their interaction as an edge of the graph.

\subsubsection*{Natural science}

Chemistry, biology, and other natural sciences have been and remain one of the main driving forces in the development of GNN. The reason for this is that these sciences pose problems closely related to data in the form of graphs - molecules and materials. Also, these problems often require significant computational power when solved by classical methods, so it is appropriate to use machine learning methods for them. In general, when comparing classical methods of machine learning and GNN, the latter show better results.

\section{Recent work}
In this paper, the problems of predicting the solubility of a substance in water based on the structure of a molecule are considered. As described above, solubility is an important property for the production of drugs, which can be calculated using experimental (and therefore not very accurate) data. Therefore, it is advisable to use a neural network to predict this property.

Since the molecule is essentially a graph, it is possible to use a graph neural network in the problem, preserving the original structure of the input data.

// TODO: дописать тут про генеративную часть задачи

\section{GNN approach}

How could working with a graph using traditional machine learning methods look like? One could take the adjacency matrix of a graph, which is a matrix of zeros and ones (for graphs without weights) or a matrix of various numbers for graphs with weights. The vertices of the graph are located vertically and horizontally of such a matrix. If there is an edge between the vertices, units (or the weight of the edge) are set at the intersection of these vertices. If the vertices are not connected by an edge, zero remains in the matrix cell.
Example of such matrix is below:

\includegraphics[width=\textwidth]{adj.png}

Intuitively, this seems to be a way out of the situation, but in fact, such a representation of the graph has a serious drawback. The vertices of the graph are not ordered, that is, it is impossible to uniquely arrange them by numbers 1, 2, 3 ... so that this order is always preserved. The vertices can be swapped (keeping the edges), while the graph will remain the same, and the adjacency matrix will change, that is, we will get one object with many different sets of features.

For this reason, classical machine learning methods are poorly suited for working with graph structures.

\subsection{Graph convolution neural networks (GCN)}
GCNs are neural networks designed to perform convolutions over undirected graph data. Let's consider the main difference from CNN.

\includegraphics[width=\textwidth]{layers.png}

Here, x is a well-structured data - maybe, image or feature vector. And in GCN, the input will be a graph. Also, instead of inferring a single $z$, it infers the value $z_i$ for each node $i$ in the graph. And to make predictions for $Z_i$, GCN utilizes both $X_i$ and its neighboring nodes in the calculation.

In GNN, single layer can be described with the following equation:
\begin{equation}
H^{l+1} = \sigma(D - \frac{1}{2}AD - \frac{1}{2}H^{(l)}W^{(l)})
\end{equation}

Here H represents the node embeddings within a graph, W represents a projection matrix, A represents the adjacency matrix of the graph with added self-connections, D represents a diagonal matrix that stores the degree of each node, and sigma represents an element-wise nonlinearity (like ReLU). The equation simply projects the node embeddings, computes each new embedding as the average of all of a node’s neighbors, and applies an element-wise nonlinearity to the new embeddings. These operations comprise a single graph convolutional layer, and these layers can be stacked to build a GCN:

\includegraphics[width=\textwidth]{gcn_web.png}

During some tests, in [KIPF-WELLING] was obtained, that the best results are obtained with a 2- or 3-layer model. Besides, with a deep GCN (more than 7 layers), it tends to get bad performances (dashed blue line). One solution is to use the residual connections between hidden layers (purple line)

\includegraphics[width=\textwidth]{layers_num.png}

\subsection{Selected model}
My work uses a GCN with three layers and a linear output. As a function of the transition between layers, the $leaky_relu$ function showed the best results in the tests. Below is the model code:

\begin{lstlisting}[language=Python]
class GCN(torch.nn.Module):
    def __init__(self, num_features, hidden_channels):
        super(GCN, self).init()
        torch.manual_seed(42)

        self.conv1 = GCNConv(num_features, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.conv3 = GCNConv(hidden_channels, hidden_channels)
        self.out = Linear(hidden_channels*2, 1)

    def forward(self, x, edge_index, batch_index):
        hidden = F.leaky_relu(self.conv1(x, edge_index))
        hidden = F.leaky_relu(self.conv2(hidden, edge_index))
        hidden = F.leaky_relu(self.conv3(hidden, edge_index))

        hidden = torch.cat([gmp(hidden, batch_index), 
                            gap(hidden, batch_index)], dim=1)

        out = self.out(hidden)
        return out, hidden
\end{lstlisting}
There are two input parameters in this model. The first, \begin{verbatim}num_features\end{verbatim} is the number of features, also this is the size of tensor with features. This parameters is obtains from dataset.
Also, the second parameter, \begin{verbatim}hidden_channels\end{verbatim} is the model's parameter and may vary. During the tests, a parameter with a value of 64 showed good results.

This model has a fairly simple architecture compared to, for example, MPNN (Message Passing Neural Network) or GAT (Graph Attention Network). She learns quite quickly, although the training time depends significantly on the size of the dataset. Metrics show a steady improvement in the loss function.

\section{Datasets}
A large number of datasets have been collected for tasks related to the study of molecules. Since this work is aimed at predicting a specific property of the molecule, accordingly, datasets related to solubility were selected.

The model was trained on the AqSol dataset [https://www.nature.com/articles/s41597-019-0151-1 ], consists of aqueous solubility values of 9,982 unique compounds, along with some relevant topological and physico-chemical 2D descriptors. 

To work with the model, it is important to have a SMILES string - one of the generally accepted formats for recording molecular structures in the form of a string, and AqSol contains such data.

There are several solubility groups in this dataset: compounds with 0 and higher solubility value (highly soluble), those in the range of 0 to -2 (soluble), those in the range of -2 to -4 (slightly soluble) and insoluble if less than -4. Image below shows the distribution of solubility values.

\includegraphics[width=\textwidth]{aqsol-distrib.png}

The ESOL (Delaney) dataset is used to test the performance of the model on data other than the training dataset. It consists of 1128 molecules presented in SMILES format and data on their solubility. The solubility distribution can be seen on the graph:

\includegraphics[width=\textwidth]{esol.png}

\section{Obtained results} 
GCN models were chosen to predict solubility. The AqSol dataset was used to train the model, and the ESOL dataset was used to test the trained model.
Parameters with which the model was trained:
$$hidden\_channels = 64$$
$$batch\_size = 128$$
$$optimizer = Adam$$
$$lr = 0.0001$$
$$metric = MSE$$
$$epochs = 3000$$

On the training dataset, the minimum value of the MSE metric corresponded to 0.44. The change in values by epoch is reflected in the graph:

\includegraphics[width=\textwidth]{train.jpg}

On the benchmark dataset, the minimum value of the MSE metric corresponded to 0.57. The change in values by epoch is reflected in the graph:

\includegraphics[width=\textwidth]{bench.jpg}

If we look for the solubility value by the formula using experimental data, the error can range from 0.5 to 1.6 logarithmic units. Therefore, such indicators for the metric are acceptable for the model and in some cases exceed the accuracy of the calculation using experimental data.

\section{Conclusion}
The paper considered the problem of predicting solubility in water by molecular structure. The substantiation of the relevance of the task is carried out. The GCN model was used for the solution, its training was performed on the AqSol dataset. The ESOL benchmark was used to test the model.

The trained model is able to predict solubility for molecules in the SMILES format.

Metrics show that the quality of the model is comparable to the error that occurs when calculating solubility based on experimental data, and in some cases the model shows better results.
\end{document} % Конец текста.

// https://users.math.msu.edu/users/weig/paper/p223.pdf
// https://habr.com/ru/company/vk/blog/557280/
// https://github.com/Dyakonov/DL/blob/master/ESSE_2021/E103_GNN.pdf
// https://drive.google.com/file/d/1Oxo4WHLjDBCNoqJX6uxf9KzdF4f1d9tO/view
// https://mlcourse.at.ispras.ru/wp-content/uploads/2020/11/%D0%9D%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5_%D1%81%D0%B5%D1%82%D0%B8_%D0%B4%D0%BB%D1%8F_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0_%D0%B3%D1%80%D0%B0%D1%84%D0%BE%D0%B2.pdf
// https://www.researchgate.net/publication/351495116_Fusion_of_text_and_graph_information_for_machine_learning_problems_on_networks
// https://www.nature.com/articles/s41597-019-0151-1
// https://www.organic-chemistry.org/prog/peo/logS.html - картинка с распределением
// http://snap.stanford.edu/class/cs224w-2021/slides/01-intro.pdf стр 60 - картинка для генерации
// https://www.researchgate.net/figure/Different-types-of-graphs-and-their-corresponding-adjacency-matrix-representations-The_fig1_347300725 - картинка для матрицы графа
// gcn https://tkipf.github.io/graph-convolutional-networks/
// Kipf-Welling https://arxiv.org/pdf/1609.02907.pdf
// https://pubmed.ncbi.nlm.nih.gov/15154768/ датасет Delaney
