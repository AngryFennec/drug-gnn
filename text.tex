\documentclass[a4paper,14pt]{article}
\linespread{1.5}

\usepackage{cmap}					% поиск в PDF
\usepackage[T2A]{fontenc}			% кодировка
\usepackage[utf8]{inputenc}			% кодировка исходного текста
\usepackage[english]{babel}	% локализация и переносы
\usepackage{listings}
\usepackage{graphicx}
\usepackage{float}
\graphicspath{ {./images/} }

\author{Anastasia Nichiporchuk}
\title{Drug Discovery via Graph Neural Networks}
\date{\today}

\begin{document}

\maketitle

\newpage
\tableofcontents
\newpage

\section*{Abstract}
For clinical testing and drug development, it is important not only to find a substance that can have the desired effect on the body, but also to pack it into a ready-for-consumption form. This is significantly influenced by the aqueous solubility of molecules. A lot of work has been devoted to the solubility search (for example, \cite{Sol1} \cite{Sol2}), but the main problem remains the accuracy of the proposed models. 

In this paper, GCN, GAT, GraphSAGE models working with graph representation of molecules are considered. The models are trained on the AqSol dataset, and the ESOL dataset is used for testing. The quality of the obtained model results is compared, the GraphSAGE model is determined as the best, and the accuracy of the models and the solubility calculation based on experimental data are compared.

\section{Introduction}\label{1}

The need to treat illness and maintain health has accompanied humans throughout their existence as a species. The development of the pharmaceutical industry has improved the quality of life, increased its duration, and made it possible to cope with diseases that were previously considered incurable. The development of vaccines for preventive protection against disease, both for the individual and for society as a whole, as demonstrated by the recent COVID-19 pandemic, is also crucial. 

Modern drug development is inextricably linked to the use of various technologies. Pharmaceutical companies compete for primacy in the development of drugs for diseases that place the greatest burden on the health care system. Before the discovery of penicillin, these were mostly diseases of an infectious nature. Nowadays, cardiovascular diseases, cancer and neurodegenerative diseases account for the largest percentage.

Because time is of the essence in drug development, pharmaceutical companies use a variety of cutting-edge research methods. In addition to laboratory research, computer simulations of different processes, mathematical modeling, and other methods that can reduce the amount of laboratory research are actively used. 


This is due to the fact that conducting any laboratory research requires time and financial expenses. Also, you need highly qualified specialists who are able to conduct this research. For example, a substance can be tested in the laboratory for a fairly small number of properties, so you need to try empirically to exclude those tests that are known to give a bad result.

With the development of computing power, as well as biotechnology and cheminformatics, it has become possible to use neural networks as such empirical methods. A neural network trained for a specific task can predict certain data and minimize unsuccessful laboratory tests.

\subsection{Solubility}\label{2}
Solubility is the maximum amount of solute that can be dissolved in a known amount of solvent at a particular temperature.

Factors affecting solubility:
\begin {itemize}
\item {temperature. Solubility depends significantly on temperature and can be changed by increasing or lowering the temperature. As a rule, the range from 20°C to 100 °C is considered suitable for dissolution in water.}

\item{pressure. Pressure has practically no effect on the solubility of solids, but for gaseous substances, solubility is directly proportional to the pressure of the gas over the liquid.}

\item{the nature of the solvent and the solute. Water belongs to polar solvents and, accordingly, dissolves polar substances better.}
\end {itemize}

Aqueous solubility, denoted by S, or its logarithm value log Is very important molecular property. Identification of
molecules with undesirable solubility in water at early stages is of great importance in drug development and in other related fields of pharmacology, since solubility affects the processes of absorption, distribution, metabolism and elimination (ADME) \cite{ADME}.

A number of existing QSPR \cite{QSPR} models developed to predict solubility show that solubility is related to experimental indicators such as melting point and separation coefficient.
However, the data obtained experimentally often contain measurement errors. This increases the difficulty of predicting solubility.

The most using formula of solubility is:
\begin {equation}
\label{e1}
 logS =  0.5 - 0.01(MP - 25) - logP,
\end{equation}
where MP is melting point and logP is the log of the octanol-water partition coefficient.

Both partition coefficient and aqueous solubility reveal how
absolute dissolves in a solvent.

Solubility of molecules in water is an important property in terms of drug development. Substances with good solubility can be incorporated into drugs "as is," while substances with low solubility require the addition of various impurities to increase the final solubility. 

The most popular forms of drugs are tablets and solutions for injection. It is not uncommon for laboratory tests to find a substance that has the desired effect (capable of acting as a medicine for a specific disease). Such a substance needs to be submitted for clinical trials already in the dosage form in which it is supposed to be taken by patients. At this stage there may be a hitch, precisely because the active ingredient is poorly soluble in water, and it is not technologically easy to make a tablet from it.

Solubility can also affect the absorption of the active ingredient and its stability while in the dosage form. Thus, it can be determined that the better the solubility of the substance in water, the easier and cheaper it will be to use to make the drug.

\subsection{Methods of calculating solubility}
There are two fundamentally different ways to obtain the solubility value of a particular substance. The first is to use experimental data and calculate using the equation (\ref{e1}).

The second way is to use various machine learning and deep learning methods. Many articles are devoted to such methods, for example, \cite{Sol5}\cite{Sol6}. The effectiveness and speed of the methods vary greatly. One of the main problems is that the molecule that is used as input has an irregular structure and is difficult to adapt to classical methods used in other fields, such as computer vision or NLP.

How can we improve the interaction of molecular data and neural networks?

\subsection{Recent work}
In this paper, the problems of predicting the solubility of a substance in water based on the structure of a molecule are considered. As described above, solubility is an important property for the production of drugs, which can be calculated using experimental (and therefore not very accurate) data. Therefore, it is advisable to use a neural network to predict this property.

Since the molecule is essentially a graph, it is possible to use a graph neural network in the problem, preserving the original structure of the input data.

Contribution of this work is to construct several GNN models and to compare those results on solubility prediction.

Let's formalize the task. The molecule is initially represented as a SMILES string \cite{SMILES}. To get a graph from it, we use the RDKit library \cite{RDKIT}, which contains tools for working with molecules, and PyG library \cite{PyG}. So we have an input data - molecular homogeneous graph $G$, and an output data - numeric value $y$, which means molecule solubility. And the problem is to find model $f(G)$:
\begin{equation}
    y = f(G).
\end{equation}
Of course, we cannot find the exact model, so we will look for such $\tilde f(G)$ so that the following condition is met:
$$
    \tilde y = \tilde f(G),
$$
$$
    Loss(y, \tilde y) \to min.
$$

We will use the MSE (Mean Squared Error) metric as Loss:
$$
    Loss(y, \tilde y) = \sqrt{y^2 -  \tilde y^2}.
$$

Thus, we defined the regression problem. 

In recent work, we will consider graph neural networks and some models: Graph Convolution Network (GCN), Graph Attention Network (GAT) and GraphSAGE model. 

To train and test models, two datasets will be used - AqSol and ESOL. 

At last, we compare results from all three models and try to explain it.
\section{Graph Neural Networks}\label{3}

Graph neural networks are a class of neural networks that allow you to apply deep learning techniques to a variety of data that can be represented as a graph. Graph data has no regular structure, like pictures (it is known exactly how many neighbors a pixel has) or texts (there is a sequence of words in a sentence). Therefore, classical neural networks are not well suited to this kind of data. At the same time, the variety of such data in the real world is very large - as an example, we can consider social networks, routes, and molecular structures. 

There are several different types of graph problems:
\begin{enumerate}
  \item node-level prediction
  \item edge-level prediction
  \item graph-level prediction
  \item community detection (the problem of searching for clusters in a graph).
\end{enumerate}

The first type can include various tasks of prediction and classification of graph nodes, for example, prediction of the topic of an article on the basis of citations. 

The second type includes tasks related to a pair of nodes or edges, the so-called "link prediction" tasks. As an example of such a problem, we can consider recommendation systems that predict whether a user is suitable for a service offered. 

The third type, graph prediction, involves problems where a graph is treated as an object and the output is an estimate of its property. These can be both classification and regression problems. Such problems are often used when considering molecules to predict their properties.

Clustering tasks belong to unsupervised learning and can be used, for example, to distinguish groups of familiar users in social networks.

Below we will consider in more detail in which areas graph neural networks are used.

\subsection{Graph neural networks in natural science}\label{4}

Chemistry, biology, and other natural sciences have been and remain one of the main driving forces in the development of GNN. The reason for this is that these sciences pose problems closely related to data in the form of graphs - molecules and materials. Also, these problems often require significant computational power when solved by classical methods, so it is appropriate to use machine learning methods for them. In general, when comparing classical methods of machine learning and GNN, the latter show better results.

\subection{GNN approach}

How could working with a graph using traditional machine learning methods look like? One could take the adjacency matrix of a graph, which is a matrix of zeros and ones (for graphs without weights) or a matrix of various numbers for graphs with weights. The nodes of the graph are located vertically and horizontally of such a matrix. If there is an edge between the nodes, units (or the weight of the edge) are set at the intersection of these nodes. If the nodes are not connected by an edge, zero remains in the matrix cell.

Intuitively, this seems to be a way out of the situation, but in fact, such a representation of the graph has a serious drawback. The nodes of the graph are not ordered, that is, it is impossible to uniquely arrange them by numbers 1, 2, 3 ... so that this order is always preserved. The nodes can be swapped (keeping the edges), while the graph will remain the same, and the adjacency matrix will change, that is, we will get one object with many different sets of features.

For this reason, classical machine learning methods are poorly suited for working with graph structures.

\subsection{Graph convolution neural networks (GCN)}
GCNs are neural networks designed to perform convolutions over undirected graph data. Let's consider the main difference from CNN.

In CNNs, x is a well-structured data - maybe, image or feature vector. And in GCN, the input will be a graph. Also, instead of inferring a single $z$, it infers the value $z_i$ for each node $i$ in the graph. And to make predictions for $Z_i$, GCN utilizes both $X_i$ and its neighboring nodes in the calculation.

In GNN, single layer can be described with the following equation:
\begin{equation}
H^{l+1} = \sigma(D - \frac{1}{2}AD - \frac{1}{2}H^{(l)}W^{(l)})
\end{equation}

Here H represents the node embeddings within a graph, W represents a projection matrix, A represents the adjacency matrix of the graph with added self-connections, D represents a diagonal matrix that stores the degree of each node, and sigma represents an element-wise nonlinearity (like ReLU). The equation simply projects the node embeddings, computes each new embedding as the average of all of a node’s neighbors, and applies an element-wise nonlinearity to the new embeddings. These operations comprise a single graph convolutional layer, and these layers can be stacked to build a GCN.

During some tests, in \cite{GCN} was obtained, that the best results are obtained with a 2- or 3-layer model. Besides, with a deep GCN (more than 7 layers), it tends to get bad performances.

\subsection{Graph Attention Networks (GAT)}
Graph Attention Network (GAT) \cite{GAT} is based on the idea of computing the hidden representations of each node in the graph by attending over its neighbors using a self-attention strategy.

By staking layers, nodes can attend to their neighborhood node features by specifying different weights to different nodes and not needing costly matrix operation or knowing the graph structure upfront.

To compute one single GAT layer, we initialized weight matrix $W$ and define the equation to compute attention coefficient:
\begin{equation}
    e_{ij} = a(Wh_i,Wh_j), 
\end{equation}
which means the inportance of node j's features to node i. In this equation $a$ called shared attentional mechanism. Then we computes $e_{ij}$ only for neighbors and use them to compute $\alpha_{ij}$:
\begin{equation}
    \alpha_{ij} = softmax(e_{ij})
\end{equation}
In the original work \cite{GAT} authors use LeakyReLu function as attention mechanism.

The aggregated features of each node can be compute by equation:
\begin{equation}
    h_i = \sigma(\sum_{j \in neibs(i)}\alpha_{ij}Wh_j),
\end{equation}
where sigma is some nonlinear activation function. 

\subsection{GraphSAGE}
GraphSAGE \cite{SAGE} is based on SAmpling and aggreGatE, where it samples only a subset of neighboring nodes at different depth layers. It then aggregates the neighbors of the previous layers using an aggregator.

At each iteration, nodes aggregate information from their local neighbors based on sampling. Each aggregator function aggregates information from a different number of search depth, away from a given node. As a result, nodes incrementally gain more and more information from further reaches of the graph. The goal of GraphSAGE is to learn a representation for every node based on some combination of its neighbouring nodes.

GraphSAGE is that it was the first work to create inductive node embeddings in an unsupervised way. At the first stage, forward propagation occurs, which results in embeddings for the nodes of the graph. Next, aggregator functions aggregate feature information from nodes, which were embedded earlier. Then we use aggregated information to predict graph context.

This architecture using for single SAGEConv layer, and GraphSAGE neural network is usually consists of some SAGEConv layers.

\subsection{Selected models}
My work uses three models with different layers and similar architecture:
GCN, GAT and GraphSAGE with three layers and a linear output. As a function of the transition between layers, the $leaky_relu$ function showed the best results in the tests. Below is the GCN architecture:
\newline

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{gcn.jpg}
    \caption{GCN architecture}
\end{figure}

GAT and GraphSAGE are the similar, but using GATConv and SAGEConv layers.

GCN was chosen because it has a fast counting speed. This is a fairly simple, basic model that shows balanced results in both speed and accuracy.

GAT is a model that uses information both from neighboring nodes to the current one and from edges. This model is considered to compare it with GCN and GraphSAGE, which use only nodes. The speed of counting GAT is inferior to GCN, but comparable to GraphSAGE.

GraphSAGE is essentially a modified GCN model that takes into account not only current layer's features, but also each node’s own features from the previous layer during updation. GraphSAGE is powerfull, but not very fast. It was choosen to consider nodes with their neighbours to receive good computational results.

There are two input parameters in this model. The first, \begin{verbatim}num_features\end{verbatim} is the number of features, also this is the size of tensor with features. This parameters is obtains from dataset.
Also, the second parameter, \begin{verbatim}hidden_channels\end{verbatim} is the model's parameter and may vary. During the tests, a parameter with a value of 64 showed good results.

This model's arhitecture has a fairly simple architecture It learns quite quickly, although the training time depends significantly on the size of the dataset.

Parameters with which the models were trained:
$$hidden\_channels = 64$$
$$batch\_size = 64$$
$$optimizer = Adam$$
$$lr = 0.0007$$
$$metric = MSE$$
$$epochs = 3000$$

\section{Datasets}
A large number of datasets have been collected for tasks related to the study of molecules. Since this work is aimed at predicting a specific property of the molecule, accordingly, datasets related to solubility were selected.

The model was trained on the AqSol dataset \cite{AqSol}, consists of aqueous solubility values of 9,982 unique compounds, along with some relevant topological and physico-chemical 2D descriptors. 

To work with the model, it is important to have a SMILES string - one of the generally accepted formats for recording molecular structures in the form of a string, and AqSol contains such data.

There are several solubility groups in this dataset: compounds with 0 and higher solubility value (highly soluble), those in the range of 0 to -2 (soluble), those in the range of -2 to -4 (slightly soluble) and insoluble if less than -4. Image below shows the distribution of solubility values.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{aqsol.jpg}
    \caption{AqSol Solubility Distribution}
\end{figure}


The ESOL (Delaney) dataset \cite{ESOL} is used to test the performance of the model on data other than the training dataset. It consists of 1128 molecules presented in SMILES format and data on their solubility. The solubility in ESOL dataset has rather similar to AqSol distribution:

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{esol.jpg}
    \caption{ESOL Solubility Distribution}
\end{figure}

\section{Experiment results} 
GCN models were chosen to predict solubility. The AqSol dataset was used to train the model, and the ESOL dataset was used to test the trained model.

On the training dataset, the minimum value of the MSE metric corresponded to 0.254 with GraphSAGE.
\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
  & GCN & GAT & GraphSAGE \\ 
 \hline
 Min MSE & 0.398 & 0.440 & 0.254 \\  
 \hline
 Mean MSE & 0.791 & 0.804 & 0.536   \\
 \hline
\end{tabular}
\end{center}

On the benchmark dataset, the minimum value of the MSE metric corresponded to 0.281 with GraphSAGE. 
\begin{center}
\begin{tabular}{ |c|c|c|c| }
\hline
  & GCN & GAT & GraphSAGE \\ 
 \hline
 Min MSE & 0.394 & 0.469 & 0.281 \\  
 \hline
 Mean MSE & 0.485 & 0.599 &  0.406  \\
 \hline
\end{tabular}
\end{center}

GrapSAGE showed the best results on both training and test datasets. At the same time, the results were slightly better than GCN and significantly better than GAT. Perhaps this fact is explained by the fact that GrapSAGE uses deeper connections between nodes in calculations (it considers not only direct neighbors, but also neighbors of other orders). This allows you to bypass the GCN, which does not take into account the like. As for GAT, the reason for the poor results probably turned out to be that GAT uses information from nodes and edges, and edges were not so important for this task.
If we look for the solubility value by the formula using experimental data, the error can range from 0.5 to 1.6 logarithmic units. Therefore, such indicators for the metric are acceptable for the model and in some cases exceed the accuracy of the calculation using experimental data.

\section{Conclusion}
The paper considered the problem of predicting aqueous solubility by molecular structure. The substantiation of the relevance of the task is carried out. The GCN, GAT and GraphSAGE models were used for the solution, those training was performed on the AqSol dataset. The ESOL benchmark was used to test the model.

The trained models is able to predict solubility for molecules in the SMILES format.

Metrics show that the quality of the models is comparable to the error that occurs when calculating solubility based on experimental data, and in some cases the models shows better results.

If we compare the results shown by the models, then GraphSAGE showed the best minimum values for the training and test datasets and the best average values for all epochs. 

\newpage
\begin{thebibliography}{25}
\bibitem{Sol1}
Ye, Z., Ouyang, D.  (2021). \emph{Prediction of small-molecule compound solubility in organic solvents by machine learning algorithms}, J Cheminform 13, 98.
\bibitem{Sol2}
Lee, S., Lee, M., Gyak, K. W., Kim, S. D., Kim, M. J., Min, K. (2022). \emph{Novel Solubility Prediction Models: Molecular Fingerprints and Physicochemical Features vs Graph Convolutional Neural Networks.} ACS omega, 7(14), 12268–12277.
\bibitem{SMILES}
Weininger D. (1988). \emph{SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules.} J. Chem. Inf. Comput. Sci. 1988, 28, 1, 31–36.
\bibitem{RDKIT}
RDKit: Open-source cheminformatics. https://www.rdkit.org
\bibitem{GCN}
Kipf, T.N., Welling, M. (2016). \emph{Semi-Supervised Classification with Graph Convolutional Networks.} 5th International Conference on Learning Representations (ICLR-17).
\bibitem{SAGE}
Hamilton, W., Ying, Z., Leskovec, J. (2017). \emph{Inductive Representation Learning on Large Graphs}, Advances in Neural Information Processing Systems , 1024-1034.
\bibitem{GAT}
Veličković, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., Bengio, Y. (2017). \emph{Graph Attention Networks}, 6th International Conference on Learning Representations.
\bibitem{AqSol}
Sorkun, M.C., Khetan, A., Er, S. (2019). \emph{AqSolDB, a curated reference set of aqueous solubility and 2D descriptors for a diverse set of compounds.} Sci Data 6, 143.
\bibitem{ESOL}
Delaney J. S. (2004). \emph{ESOL: estimating aqueous solubility directly from molecular structure.} Journal of chemical information and computer sciences, 44(3), 1000–1005.
\bibitem{ChemInf}
Lo, Y. C., Rensi, S. E., Torng, W., & Altman, R. B. (2018). \emph{Machine learning in chemoinformatics and drug discovery.} Drug discovery today, 23(8), 1538–1546.
\bibitem{Sol3}
Deng T., Jia G. (2020). \enph{Prediction of aqueous solubility of compounds based on neural network.} Molecular Physics, 118:2.
\bibitem{PyG}
Fey, M., Lenssen, J.E. (2019). \emph{Fast Graph Representation Learning with PyTorch Geometric.}
\bibitem{MolNet}
Zhenqin W., Ramsundar B., Feinberg E., Gomes J., Geniesse C., Pappu A., Leswing K. Pande V. (2017). \emph{MoleculeNet: A Benchmark for Molecular Machine Learning.} Chemical Science. 9.
\bibitem{Sol4}
Ye, Z., Ouyang, D. (2021). \emph{Prediction of small-molecule compound solubility in organic solvents by machine learning algorithms.} J Cheminform 13, 98.
\bibitem{Sol5}
Wu K., Zhao Z., Wang R., Wei G. (2017). \emph{TopP-S: Persistent homology based multi-task deep neural networks for simultaneous predictions of partition coefficient and aqueous solubility.} Journal of Computational Chemistry. 39.
\bibitem{Sol6}
Wieder O., Kuenemann M., Wieder M., Seidel T., Meyer C., Langer T. (2021). \emph{Improved Lipophilicity and Aqueous Solubility Prediction With Composite Graph Neural Networks.} 
\bibitem{Sol7}
Waqar A., Hilal T., Kil T.C. (2023). \emph{Attention-Based Graph Neural Network for Molecular Solubility Prediction.} ACS Omega 2023 8 (3), 3236-3244.
\bibitem{Sol8}
Di, L.; Fish, P. V.; Mano, T. (2012). \emph{Bridging Solubility between Drug Discovery and Development.} Drug Discovery Today 2012, 17 (9–10), 486–495.
\bibitem{ADME}
Mandlik V., Bejugam P.R., Singh S. (2016). \emph{Chapter 6 - Application of Artificial Neural Networks in Modern Drug Discovery.} Artificial Neural Network for Drug Design, Delivery and Disposition, Academic Press. Pages 123-139.
\bibitem{QSPR}
Meftahi N., Walker M.L., Smith B.J. (2021). \emph{Predicting aqueous solubility by QSPR modeling.} Journal of Molecular Graphics and Modelling, Volume 106.

\end{thebibliography}


\end{document}
